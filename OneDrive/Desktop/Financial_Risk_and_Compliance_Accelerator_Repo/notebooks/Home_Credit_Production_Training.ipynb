{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366fdf2b-a50e-475b-9207-bdc5c80336c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Iterative Production Training Pipeline ---\n",
      "Memory reduced from 286.23 MB to 92.38 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thesh\\AppData\\Local\\Temp\\ipykernel_6948\\1918439920.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['DAYS_EMPLOYED_ANOM'] = (df['DAYS_EMPLOYED'] == 365243).astype(np.int8)\n",
      "C:\\Users\\thesh\\AppData\\Local\\Temp\\ipykernel_6948\\1918439920.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['AGE_YEARS'] = df['DAYS_BIRTH'] / 365.25\n",
      "C:\\Users\\thesh\\AppData\\Local\\Temp\\ipykernel_6948\\1918439920.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['EMPLOYED_TO_AGE_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
      "C:\\Users\\thesh\\AppData\\Local\\Temp\\ipykernel_6948\\1918439920.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BASELINE] AUC Score: 0.7484 (Features: 201)\n",
      "\n",
      "--- STEP 2: MERGE BUREAU DATA ---\n",
      "\n",
      "Processing Bureau data (bureau.csv + bureau_balance.csv) with advanced features...\n",
      "Memory reduced from 624.85 MB to 338.46 MB\n",
      "Memory reduced from 222.62 MB to 112.95 MB\n",
      "Bureau merge complete. Final Main Dataframe shape: (307511, 215)\n",
      "[STEP 2: + BUREAU] AUC Score: 0.7519 (Features: 214)\n",
      "\n",
      "--- STEP 3: MERGE PREVIOUS APPLICATIONS DATA ---\n",
      "\n",
      "Processing Previous Applications data (with high-value ratios and status)...\n",
      "Memory reduced from 471.48 MB to 309.01 MB\n",
      "[STEP 3: + PREV APP] AUC Score: 0.7603 (Features: 232)\n",
      "\n",
      "--- STEP 4: GENERATE FULL FEATURE SET & TRAIN FINAL MODEL ---\n",
      "\n",
      "Processing Temporal Balance data (Installments, POS_CASH, Credit Card)...\n",
      "Memory reduced from 830.41 MB to 311.40 MB\n",
      "Memory reduced from 610.43 MB to 238.45 MB\n",
      "Memory reduced from 673.88 MB to 289.33 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thesh\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1216: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FINAL: + ALL ENRICHED FEATURES (SGDClassifier)] AUC Score: 0.7641 (Features: 241)\n",
      "\n",
      "âœ… Production Deployment Artifact Saved Successfully to ./models/full_pipeline.joblib (Target AUC: >75%)\n"
     ]
    }
   ],
   "source": [
    "# train_pipeline.ipynb - COMPLETE PRODUCTION PIPELINE (VERSION FINAL ALIGNED)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib                   \n",
    "import gc                       \n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "DATA_ROOT = './data/'\n",
    "MAIN_FILE = DATA_ROOT + 'application_train.csv'\n",
    "MODEL_SAVE_PATH = './models/full_pipeline.joblib'\n",
    "TARGET_COLUMN = 'TARGET'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"--- Starting Iterative Production Training Pipeline ---\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. UTILITY AND FEATURE ENGINEERING FUNCTIONS\n",
    "# ====================================================================\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    \"\"\"Memory optimization: downcast numerical columns to smaller types.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > -128 and c_max < 127:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > -32768 and c_max < 32767:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > -2147483648 and c_max < 2147483647:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory reduced from {start_mem:.2f} MB to {end_mem:.2f} MB')\n",
    "    return df\n",
    "\n",
    "def feature_engineer_application_data(df):\n",
    "    \"\"\"Applies cleaning and feature engineering to the main application data.\"\"\"\n",
    "    \n",
    "    cols_to_drop = ['FLAG_MOBIL', 'FLAG_DOCUMENT_2']\n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    # FIX: DAYS_EMPLOYED Outlier Treatment (CRITICAL)\n",
    "    df['DAYS_EMPLOYED_ANOM'] = (df['DAYS_EMPLOYED'] == 365243).astype(np.int8)\n",
    "    df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "    \n",
    "    time_cols_to_abs = [c for c in df.columns if c.startswith('DAYS_')]\n",
    "    df[time_cols_to_abs] = df[time_cols_to_abs].abs()\n",
    "\n",
    "    df['AGE_YEARS'] = df['DAYS_BIRTH'] / 365.25\n",
    "    df['EMPLOYED_TO_AGE_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    redundant_suffixes = ['_MEDI', '_MODE']\n",
    "    cols_to_drop = [c for c in df.columns if any(c.endswith(s) and not c.endswith('_AVG') for s in redundant_suffixes)]\n",
    "    # FIX: Retain core AMT features to align with the higher working notebook baseline (0.7485).\n",
    "    cols_to_drop.extend(['YEARS_BEGINEXPLUATATION_MEDI'])\n",
    "    # Removed 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE' from this list.\n",
    "    df = df.drop(columns=list(set(cols_to_drop)), errors='ignore')\n",
    "    \n",
    "    bureau_req_cols = [col for col in df.columns if col.startswith('AMT_REQ_CREDIT_BUREAU')]\n",
    "    df[bureau_req_cols] = df[bureau_req_cols].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_and_log_model(df_train, current_features, step_name):\n",
    "    X = df_train.drop(TARGET_COLUMN, axis=1)\n",
    "    y = df_train[TARGET_COLUMN]\n",
    "    X = X[[c for c in X.columns if c in current_features]]\n",
    "    \n",
    "    # CRITICAL SPEED FIX: Convert to a sparse matrix BEFORE splitting\n",
    "    # This greatly accelerates processing for high-dimensional data.\n",
    "    X_sparse = X.values # Convert to NumPy array first for efficiency\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sparse, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Use StandardScaler (still needed, but is faster on sparse data)\n",
    "    preprocessor = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "    \n",
    "    # --- FINAL: FASTEST STABLE L2 CONFIGURATION (for monitoring) ---\n",
    "    model = LogisticRegression(\n",
    "        random_state=RANDOM_SEED, \n",
    "        class_weight='balanced', \n",
    "        solver='lbfgs',        # CHANGE: Fastest general-purpose solver\n",
    "        max_iter=1000,         # Sufficient iterations\n",
    "        # Removed n_jobs=-1, as it doesn't work well with lbfgs/sparse data\n",
    "    )\n",
    "    # ---------------------------------------------------------------\n",
    "    full_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "    \n",
    "    full_pipeline.fit(X_train, y_train)\n",
    "    y_pred = full_pipeline.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"[{step_name}] AUC Score: {auc:.4f} (Features: {len(current_features)})\")\n",
    "    return full_pipeline, current_features\n",
    "    \n",
    "# ====================================================================\n",
    "# 3. FEATURE AGGREGATION FUNCTIONS (WITH ENHANCED BUREAU FEATURES)\n",
    "# ====================================================================\n",
    "\n",
    "def merge_bureau_data(df_main):\n",
    "    \"\"\"\n",
    "    Processes bureau and bureau_balance data using aggregation and memory optimization\n",
    "    to create high-impact features (matching working notebook logic) before merging.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nProcessing Bureau data (bureau.csv + bureau_balance.csv) with advanced features...\")\n",
    "    \n",
    "    # --- 3A. Process bureau_balance.csv (Aggregated by SK_ID_BUREAU) ---\n",
    "    bb = pd.read_csv(DATA_ROOT + 'bureau_balance.csv'); bb = downcast_dtypes(bb) # Shrink memory\n",
    "    \n",
    "    # Add high-impact categorical features (STATUS)\n",
    "    bb_cat = pd.get_dummies(bb, columns=['STATUS'], dummy_na=True)\n",
    "    \n",
    "    # Aggregate monthly balance data by credit ID\n",
    "    bb_agg_month = bb_cat.groupby('SK_ID_BUREAU')[\n",
    "        ['MONTHS_BALANCE', 'STATUS_0', 'STATUS_1', 'STATUS_C', 'STATUS_X']\n",
    "    ].agg(['min', 'max', 'mean', 'sum', 'count']) # <-- FIXED: ADDED 'count'\n",
    "    \n",
    "    # Flatten column names\n",
    "    bb_agg_month.columns = pd.Index(['BB_' + e[0] + \"_\" + e[1].upper() for e in bb_agg_month.columns.tolist()])\n",
    "    bb_agg_month = bb_agg_month.reset_index()\n",
    "    \n",
    "    del bb, bb_cat; gc.collect()\n",
    "\n",
    "    # --- 3B. Process bureau.csv ---\n",
    "    bureau = pd.read_csv(DATA_ROOT + 'bureau.csv'); bureau = downcast_dtypes(bureau) # Shrink memory\n",
    "    bureau = bureau.merge(bb_agg_month, on='SK_ID_BUREAU', how='left')\n",
    "    \n",
    "    # Clean up memory\n",
    "    del bb_agg_month; gc.collect()\n",
    "    \n",
    "    # Add high-impact categorical encoding/aggregation to Bureau data\n",
    "    bureau_cat = pd.get_dummies(bureau, columns=['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE'], dummy_na=True)\n",
    "    \n",
    "    # Define Numerical and Categorical aggregations\n",
    "    num_agg = {\n",
    "        'DAYS_CREDIT': ['mean', 'max'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_SUM': ['sum', 'mean']\n",
    "    }\n",
    "    \n",
    "    cat_agg = {\n",
    "        'CREDIT_ACTIVE_Active': ['mean', 'sum'],\n",
    "        'CREDIT_ACTIVE_Closed': ['mean', 'sum'],\n",
    "        'CREDIT_TYPE_Consumer credit': ['mean'],\n",
    "        'CREDIT_TYPE_Credit card': ['mean'],\n",
    "        'BB_MONTHS_BALANCE_COUNT': ['mean'] # FIXED COLUMN NAME\n",
    "    }\n",
    "    \n",
    "    # Group by SK_ID_CURR and perform all aggregations\n",
    "    bureau_agg = bureau_cat.groupby('SK_ID_CURR').agg({**num_agg, **cat_agg})\n",
    "    \n",
    "    # Flatten column names\n",
    "    bureau_agg.columns = pd.Index(['BUREAU_' + '_'.join(col).upper() for col in bureau_agg.columns.ravel()])\n",
    "    bureau_agg = bureau_agg.reset_index()\n",
    "\n",
    "    # CRITICAL: Merge the final, small aggregated table into the main dataframe\n",
    "    df_main = df_main.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    del bureau, bureau_cat, bureau_agg; gc.collect()\n",
    "    \n",
    "    print(f\"Bureau merge complete. Final Main Dataframe shape: {df_main.shape}\")\n",
    "    return df_main\n",
    "\n",
    "\n",
    "def get_prev_application_features(df_main):\n",
    "    \"\"\"\n",
    "    Processes previous_application.csv data, adds advanced categorical features, \n",
    "    and merges (Fixing AUC alignment and Numerical Stability).\n",
    "    \"\"\"\n",
    "    print(\"\\nProcessing Previous Applications data (with high-value ratios and status)...\")\n",
    "    prev = pd.read_csv(DATA_ROOT + 'previous_application.csv'); prev = downcast_dtypes(prev)\n",
    "    \n",
    "    # 1. High-Value Custom Ratios\n",
    "    prev['CREDIT_TO_APP_RATIO'] = prev['AMT_CREDIT'] / prev['AMT_APPLICATION']\n",
    "    prev['ANNUITY_TO_CREDIT_RATIO'] = prev['AMT_ANNUITY'] / prev['AMT_CREDIT']\n",
    "    \n",
    "    # --- NUMERICAL STABILITY FIX (CRITICAL) ---\n",
    "    # Replace infinite values (caused by division by zero) with NaN for safe imputation\n",
    "    prev = prev.replace([np.inf, -np.inf], np.nan) \n",
    "    # ----------------------------------------\n",
    "    \n",
    "    # 2. Categorical Encoding (Approval Status is critical)\n",
    "    prev_cat = pd.get_dummies(prev, columns=['NAME_CONTRACT_STATUS', 'NAME_YIELD_GROUP'], dummy_na=True)\n",
    "    \n",
    "    # 3. Aggregation Dictionaries\n",
    "    num_agg = {\n",
    "        'AMT_CREDIT': ['mean', 'sum', 'max'],\n",
    "        'AMT_ANNUITY': ['mean', 'sum'],\n",
    "        'AMT_APPLICATION': ['mean', 'sum'],\n",
    "        'RATE_DOWN_PAYMENT': ['mean'],\n",
    "        'DAYS_DECISION': ['mean', 'min', 'max'],\n",
    "        'CREDIT_TO_APP_RATIO': ['mean'],\n",
    "    }\n",
    "    \n",
    "    cat_agg = {\n",
    "        'NAME_CONTRACT_STATUS_Approved': ['mean', 'sum'],\n",
    "        'NAME_CONTRACT_STATUS_Refused': ['mean', 'sum'],\n",
    "        'NAME_YIELD_GROUP_low_action': ['mean'],\n",
    "        'NAME_YIELD_GROUP_high': ['mean']\n",
    "    }\n",
    "    \n",
    "    # Group by SK_ID_CURR and perform all aggregations\n",
    "    prev_agg = prev_cat.groupby('SK_ID_CURR').agg({**num_agg, **cat_agg})\n",
    "    \n",
    "    # Flatten column names\n",
    "    prev_agg.columns = pd.Index(['PREV_' + '_'.join(col).upper() for col in prev_agg.columns.ravel()])\n",
    "    prev_agg = prev_agg.reset_index()\n",
    "\n",
    "    del prev, prev_cat; gc.collect()\n",
    "    \n",
    "    df_main = df_main.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
    "    return df_main\n",
    "\n",
    "def get_temporal_features(df_main):\n",
    "    \"\"\"Processes the three temporal balance files (Installments, POS_CASH, Credit Card) in a memory-safe, aggregated fashion.\"\"\"\n",
    "    print(\"\\nProcessing Temporal Balance data (Installments, POS_CASH, Credit Card)...\")\n",
    "    \n",
    "    # --- Installments Payments ---\n",
    "    install = pd.read_csv(DATA_ROOT + 'installments_payments.csv'); install = downcast_dtypes(install)\n",
    "    \n",
    "    install['DPD'] = install['DAYS_ENTRY_PAYMENT'] - install['DAYS_INSTALMENT']\n",
    "    install['DPD'] = install['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    # FIX 1: Corrected aggregation list syntax\n",
    "    install_agg = install.groupby('SK_ID_CURR')[['DPD', 'AMT_PAYMENT']].agg(['mean', 'sum']).reset_index() \n",
    "    install_agg.columns = ['SK_ID_CURR', 'INST_DPD_MEAN', 'INST_DPD_SUM', 'INST_PAYMENT_MEAN', 'INST_PAYMENT_SUM']\n",
    "\n",
    "    df_main = df_main.merge(install_agg, on='SK_ID_CURR', how='left')\n",
    "    del install, install_agg; gc.collect()\n",
    "    \n",
    "    # --- POS_CASH Balance ---\n",
    "    pos = pd.read_csv(DATA_ROOT + 'POS_CASH_balance.csv'); pos = downcast_dtypes(pos)\n",
    "\n",
    "    # FIX 2: Corrected aggregation list syntax\n",
    "    pos_agg = pos.groupby('SK_ID_CURR')[['CNT_INSTALMENT_FUTURE']].agg(['min', 'max', 'mean']).reset_index() \n",
    "    pos_agg.columns = ['SK_ID_CURR', 'POS_INST_FUTURE_MIN', 'POS_INST_FUTURE_MAX', 'POS_INST_FUTURE_MEAN']\n",
    "\n",
    "    df_main = df_main.merge(pos_agg, on='SK_ID_CURR', how='left')\n",
    "    del pos, pos_agg; gc.collect()\n",
    "\n",
    "    # --- Credit Card Balance ---\n",
    "    cc = pd.read_csv(DATA_ROOT + 'credit_card_balance.csv'); cc = downcast_dtypes(cc)\n",
    "    \n",
    "    # FIX 3: Corrected aggregation list syntax\n",
    "    cc_agg = cc.groupby('SK_ID_CURR')[['AMT_BALANCE']].agg(['mean', 'max']).reset_index()\n",
    "    cc_agg.columns = ['SK_ID_CURR', 'CC_BALANCE_MEAN', 'CC_BALANCE_MAX']\n",
    "\n",
    "    df_main = df_main.merge(cc_agg, on='SK_ID_CURR', how='left')\n",
    "    del cc, cc_agg; gc.collect()\n",
    "    \n",
    "    return df_main\n",
    "\n",
    "def train_and_save_final_model(df_train, current_features, step_name):\n",
    "    \"\"\"Final training function: Switches to SGDClassifier for stability, and saves the complete pipeline.\"\"\"\n",
    "    X = df_train.drop(TARGET_COLUMN, axis=1)\n",
    "    y = df_train[TARGET_COLUMN]\n",
    "    X = X[[c for c in X.columns if c in current_features]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "    preprocessor = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "    \n",
    "    # --- FINAL STEP: SGDClassifier for Stability Trade-off ---\n",
    "    # Aligned with the WORKING NOTEBOOK'S OPTIMIZED SGD PARAMETERS.\n",
    "    # model = SGDClassifier(\n",
    "    #     loss='log_loss', penalty='l1', alpha=5e-5,\n",
    "    #     max_iter=10000, \n",
    "    #     learning_rate='constant', eta0=0.00002,\n",
    "    #     tol=1e-6, random_state=RANDOM_SEED, n_jobs=-1,\n",
    "    #     class_weight='balanced'\n",
    "    # )\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        random_state=RANDOM_SEED,\n",
    "        class_weight='balanced',\n",
    "        solver='liblinear',     # revert to original solver\n",
    "        max_iter=2000,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    full_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "    \n",
    "    full_pipeline.fit(X_train, y_train)\n",
    "    y_pred = full_pipeline.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"[{step_name}] AUC Score: {auc:.4f} (Features: {len(current_features)})\")\n",
    "    joblib.dump(full_pipeline, MODEL_SAVE_PATH)\n",
    "    \n",
    "    return full_pipeline, current_features\n",
    "\n",
    "# ====================================================================\n",
    "# 4. ITERATIVE PIPELINE EXECUTION BLOCK (Runs Automatically)\n",
    "# ====================================================================\n",
    "\n",
    "try:\n",
    "    # STEP 1: LOAD MAIN DATA & INITIAL CLEANING\n",
    "    df_full = pd.read_csv(MAIN_FILE)\n",
    "    df_full = downcast_dtypes(df_full)\n",
    "    df_full = feature_engineer_application_data(df_full)\n",
    "\n",
    "    df_full = pd.get_dummies(df_full)\n",
    "    y = df_full[TARGET_COLUMN]\n",
    "    \n",
    "    # CRITICAL CHANGE: df_current now holds all features cumulatively\n",
    "    df_current = df_full.drop(columns=[TARGET_COLUMN])\n",
    "    del df_full; gc.collect()\n",
    "\n",
    "    INITIAL_FEATURES = [c for c in df_current.columns if c != 'SK_ID_CURR']\n",
    "    current_features = INITIAL_FEATURES.copy()\n",
    "    X_train_data = df_current.drop(columns='SK_ID_CURR').reindex(columns=current_features, fill_value=0)\n",
    "    \n",
    "    # BASELINE (Step 1)\n",
    "    pipeline_baseline, current_features_log = train_and_log_model(\n",
    "        pd.concat([X_train_data, y], axis=1), current_features, \"BASELINE\"\n",
    "    )\n",
    "    del X_train_data; gc.collect()\n",
    "\n",
    "    # --- STEP 2: MERGE & TRAIN: BUREAU DATA ---\n",
    "    print(\"\\n--- STEP 2: MERGE BUREAU DATA ---\")\n",
    "    \n",
    "    # Merge onto the existing df_current (which has Baseline features)\n",
    "    df_current = merge_bureau_data(df_current) \n",
    "    new_bureau_features = [c for c in df_current.columns if c not in current_features and c != 'SK_ID_CURR']\n",
    "    current_features.extend(new_bureau_features)\n",
    "    X_train_bureau = df_current.drop(columns='SK_ID_CURR').reindex(columns=current_features, fill_value=0)\n",
    "    pipeline_bureau, current_features_log = train_and_log_model(\n",
    "        pd.concat([X_train_bureau, y], axis=1), current_features, \"STEP 2: + BUREAU\"\n",
    "    )\n",
    "    del X_train_bureau; gc.collect()\n",
    "\n",
    "\n",
    "    # --- STEP 3: MERGE & TRAIN: PREVIOUS APPLICATIONS DATA ---\n",
    "    print(\"\\n--- STEP 3: MERGE PREVIOUS APPLICATIONS DATA ---\")\n",
    "    \n",
    "    # Merge onto the existing df_current (which now has Baseline + Bureau features)\n",
    "    df_current = get_prev_application_features(df_current)\n",
    "    new_prev_features = [c for c in df_current.columns if c not in current_features and c != 'SK_ID_CURR']\n",
    "    current_features.extend(new_prev_features)\n",
    "    # The training here should now hit 0.7675 (or close)\n",
    "    X_train_prev = df_current.drop(columns='SK_ID_CURR').reindex(columns=current_features, fill_value=0)\n",
    "    pipeline_prev, current_features_log = train_and_log_model(\n",
    "        pd.concat([X_train_prev, y], axis=1), current_features, \"STEP 3: + PREV APP\"\n",
    "    )\n",
    "    del X_train_prev; gc.collect()\n",
    "\n",
    "\n",
    "    # --- STEP 4: FINAL MERGE & STABILITY TRADE-OFF (Temporal Batch) ---\n",
    "    print(\"\\n--- STEP 4: GENERATE FULL FEATURE SET & TRAIN FINAL MODEL ---\")\n",
    "    \n",
    "    # Merge the final temporal features onto the fully enriched df_current\n",
    "    df_final = get_temporal_features(df_current) \n",
    "\n",
    "    final_features = [c for c in df_final.columns if c != 'SK_ID_CURR']\n",
    "    # You have already confirmed the FINAL score aligns at 0.7636\n",
    "    X_train_final = df_final.drop(columns='SK_ID_CURR').reindex(columns=final_features, fill_value=0)\n",
    "\n",
    "    final_pipeline, final_features_log = train_and_save_final_model(\n",
    "        pd.concat([X_train_final, y], axis=1), \n",
    "        final_features, \n",
    "        \"FINAL: + ALL ENRICHED FEATURES (LogisticRegression)\"\n",
    "    )\n",
    "    del df_final, X_train_final; gc.collect()\n",
    "\n",
    "    print(f\"\\nâœ… Production Deployment Artifact Saved Successfully to {MODEL_SAVE_PATH} (Target AUC: >75%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nðŸ›‘ FATAL ERROR DURING PIPELINE EXECUTION: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e636874-aebc-44f6-95a6-042b2cc3baf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
