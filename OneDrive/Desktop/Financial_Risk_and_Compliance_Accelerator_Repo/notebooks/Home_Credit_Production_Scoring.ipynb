{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64367544-3c50-46a3-83e6-8949d4d81ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Production Scoring Pipeline ---\n",
      "Memory reduced from 45.00 MB to 14.60 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thesh\\AppData\\Local\\Temp\\ipykernel_14656\\2151353309.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['DAYS_EMPLOYED_ANOM'] = (df['DAYS_EMPLOYED'] == 365243).astype(np.int8)\n",
      "C:\\Users\\thesh\\AppData\\Local\\Temp\\ipykernel_14656\\2151353309.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['AGE_YEARS'] = df['DAYS_BIRTH'] / 365.25\n",
      "C:\\Users\\thesh\\AppData\\Local\\Temp\\ipykernel_14656\\2151353309.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['EMPLOYED_TO_AGE_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
      "C:\\Users\\thesh\\AppData\\Local\\Temp\\ipykernel_14656\\2151353309.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 2: GENERATE FULL FEATURE SET FOR SCORING ---\n",
      "\n",
      "Processing Bureau data (bureau.csv + bureau_balance.csv) with advanced features...\n",
      "Memory reduced from 624.85 MB to 338.46 MB\n",
      "Memory reduced from 222.62 MB to 112.95 MB\n",
      "Bureau merge complete. Final Main Dataframe shape: (48744, 212)\n",
      "\n",
      "Processing Previous Applications data (with high-value ratios and status)...\n",
      "Memory reduced from 471.48 MB to 309.01 MB\n",
      "\n",
      "Processing Temporal Balance data (Installments, POS_CASH, Credit Card)...\n",
      "Memory reduced from 830.41 MB to 311.40 MB\n",
      "Memory reduced from 610.43 MB to 238.45 MB\n",
      "Memory reduced from 673.88 MB to 289.33 MB\n",
      "\n",
      "--- STEP 3: LOAD MODEL AND SCORE DATA ---\n",
      "\n",
      "Loading final production model from ./models/full_pipeline.joblib...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thesh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Production Scoring Complete. 48744 risk scores generated.\n",
      "Results saved to ./submission/credit_risk_scores.csv (File: credit_risk_scores.csv)\n"
     ]
    }
   ],
   "source": [
    "# Home_Credit_Production_Notebook.ipynb - COMPLETE PRODUCTION SCORING PIPELINE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib                         \n",
    "import gc\n",
    "import os # <--- NEW: Import os module for file system operations\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Removed unused imports: SGDClassifier, LogisticRegression, roc_auc_score, train_test_split\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "DATA_ROOT = './data/'\n",
    "MAIN_FILE = DATA_ROOT + 'application_test.csv'  # CRITICAL: Using test data for scoring\n",
    "MODEL_SAVE_PATH = './models/full_pipeline.joblib'\n",
    "TARGET_COLUMN = 'TARGET' \n",
    "RANDOM_SEED = 42\n",
    "OUTPUT_PATH = './submission/credit_risk_scores.csv' # Output file for final scores\n",
    "\n",
    "print(\"--- Starting Production Scoring Pipeline ---\")\n",
    "\n",
    "# ====================================================================\n",
    "# 2. UTILITY AND FEATURE ENGINEERING FUNCTIONS (REMAIN UNCHANGED)\n",
    "# ====================================================================\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    \"\"\"Memory optimization: downcast numerical columns to smaller types.\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > -128 and c_max < 127:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > -32768 and c_max < 32767:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > -2147483648 and c_max < 2147483647:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory reduced from {start_mem:.2f} MB to {end_mem:.2f} MB')\n",
    "    return df\n",
    "\n",
    "def feature_engineer_application_data(df):\n",
    "    \"\"\"Applies cleaning and feature engineering to the main application data.\"\"\"\n",
    "    \n",
    "    cols_to_drop = ['FLAG_MOBIL', 'FLAG_DOCUMENT_2']\n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    # FIX: DAYS_EMPLOYED Outlier Treatment (CRITICAL)\n",
    "    df['DAYS_EMPLOYED_ANOM'] = (df['DAYS_EMPLOYED'] == 365243).astype(np.int8)\n",
    "    df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "    \n",
    "    time_cols_to_abs = [c for c in df.columns if c.startswith('DAYS_')]\n",
    "    df[time_cols_to_abs] = df[time_cols_to_abs].abs()\n",
    "\n",
    "    df['AGE_YEARS'] = df['DAYS_BIRTH'] / 365.25\n",
    "    df['EMPLOYED_TO_AGE_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    redundant_suffixes = ['_MEDI', '_MODE']\n",
    "    cols_to_drop = [c for c in df.columns if any(c.endswith(s) and not c.endswith('_AVG') for s in redundant_suffixes)]\n",
    "    cols_to_drop.extend(['YEARS_BEGINEXPLUATATION_MEDI'])\n",
    "    df = df.drop(columns=list(set(cols_to_drop)), errors='ignore')\n",
    "    \n",
    "    bureau_req_cols = [col for col in df.columns if col.startswith('AMT_REQ_CREDIT_BUREAU')]\n",
    "    df[bureau_req_cols] = df[bureau_req_cols].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ====================================================================\n",
    "# 3. FEATURE AGGREGATION FUNCTIONS (REMAIN UNCHANGED)\n",
    "# ====================================================================\n",
    "\n",
    "def merge_bureau_data(df_main):\n",
    "    \"\"\"Processes bureau data and merges.\"\"\"\n",
    "    print(\"\\nProcessing Bureau data (bureau.csv + bureau_balance.csv) with advanced features...\")\n",
    "    bb = pd.read_csv(DATA_ROOT + 'bureau_balance.csv'); bb = downcast_dtypes(bb)\n",
    "    bb_cat = pd.get_dummies(bb, columns=['STATUS'], dummy_na=True)\n",
    "    bb_agg_month = bb_cat.groupby('SK_ID_BUREAU')[\n",
    "        ['MONTHS_BALANCE', 'STATUS_0', 'STATUS_1', 'STATUS_C', 'STATUS_X']\n",
    "    ].agg(['min', 'max', 'mean', 'sum', 'count'])\n",
    "    bb_agg_month.columns = pd.Index(['BB_' + e[0] + \"_\" + e[1].upper() for e in bb_agg_month.columns.tolist()])\n",
    "    bb_agg_month = bb_agg_month.reset_index()\n",
    "    del bb, bb_cat; gc.collect()\n",
    "\n",
    "    bureau = pd.read_csv(DATA_ROOT + 'bureau.csv'); bureau = downcast_dtypes(bureau)\n",
    "    bureau = bureau.merge(bb_agg_month, on='SK_ID_BUREAU', how='left')\n",
    "    del bb_agg_month; gc.collect()\n",
    "    \n",
    "    bureau_cat = pd.get_dummies(bureau, columns=['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE'], dummy_na=True)\n",
    "    num_agg = {\n",
    "        'DAYS_CREDIT': ['mean', 'max'], 'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'], 'AMT_CREDIT_SUM': ['sum', 'mean']\n",
    "    }\n",
    "    cat_agg = {\n",
    "        'CREDIT_ACTIVE_Active': ['mean', 'sum'], 'CREDIT_ACTIVE_Closed': ['mean', 'sum'], \n",
    "        'CREDIT_TYPE_Consumer credit': ['mean'], 'CREDIT_TYPE_Credit card': ['mean'], \n",
    "        'BB_MONTHS_BALANCE_COUNT': ['mean'] \n",
    "    }\n",
    "    bureau_agg = bureau_cat.groupby('SK_ID_CURR').agg({**num_agg, **cat_agg})\n",
    "    bureau_agg.columns = pd.Index(['BUREAU_' + '_'.join(col).upper() for col in bureau_agg.columns.ravel()])\n",
    "    bureau_agg = bureau_agg.reset_index()\n",
    "\n",
    "    df_main = df_main.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "    del bureau, bureau_cat, bureau_agg; gc.collect()\n",
    "    \n",
    "    print(f\"Bureau merge complete. Final Main Dataframe shape: {df_main.shape}\")\n",
    "    return df_main\n",
    "\n",
    "\n",
    "def get_prev_application_features(df_main):\n",
    "    \"\"\"Processes previous_application data and merges.\"\"\"\n",
    "    print(\"\\nProcessing Previous Applications data (with high-value ratios and status)...\")\n",
    "    prev = pd.read_csv(DATA_ROOT + 'previous_application.csv'); prev = downcast_dtypes(prev)\n",
    "    prev['CREDIT_TO_APP_RATIO'] = prev['AMT_CREDIT'] / prev['AMT_APPLICATION']\n",
    "    prev['ANNUITY_TO_CREDIT_RATIO'] = prev['AMT_ANNUITY'] / prev['AMT_CREDIT']\n",
    "    prev = prev.replace([np.inf, -np.inf], np.nan) \n",
    "    \n",
    "    prev_cat = pd.get_dummies(prev, columns=['NAME_CONTRACT_STATUS', 'NAME_YIELD_GROUP'], dummy_na=True)\n",
    "    num_agg = {\n",
    "        'AMT_CREDIT': ['mean', 'sum', 'max'], 'AMT_ANNUITY': ['mean', 'sum'],\n",
    "        'AMT_APPLICATION': ['mean', 'sum'], 'RATE_DOWN_PAYMENT': ['mean'],\n",
    "        'DAYS_DECISION': ['mean', 'min', 'max'], 'CREDIT_TO_APP_RATIO': ['mean'],\n",
    "    }\n",
    "    cat_agg = {\n",
    "        'NAME_CONTRACT_STATUS_Approved': ['mean', 'sum'], 'NAME_CONTRACT_STATUS_Refused': ['mean', 'sum'],\n",
    "        'NAME_YIELD_GROUP_low_action': ['mean'], 'NAME_YIELD_GROUP_high': ['mean']\n",
    "    }\n",
    "    prev_agg = prev_cat.groupby('SK_ID_CURR').agg({**num_agg, **cat_agg})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + '_'.join(col).upper() for col in prev_agg.columns.ravel()])\n",
    "    prev_agg = prev_agg.reset_index()\n",
    "\n",
    "    del prev, prev_cat; gc.collect()\n",
    "    df_main = df_main.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
    "    return df_main\n",
    "\n",
    "def get_temporal_features(df_main):\n",
    "    \"\"\"Processes the three temporal balance files and merges.\"\"\"\n",
    "    print(\"\\nProcessing Temporal Balance data (Installments, POS_CASH, Credit Card)...\")\n",
    "    \n",
    "    # Installments Payments\n",
    "    install = pd.read_csv(DATA_ROOT + 'installments_payments.csv'); install = downcast_dtypes(install)\n",
    "    install['DPD'] = install['DAYS_ENTRY_PAYMENT'] - install['DAYS_INSTALMENT']\n",
    "    install['DPD'] = install['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    install_agg = install.groupby('SK_ID_CURR')[['DPD', 'AMT_PAYMENT']].agg(['mean', 'sum']).reset_index() \n",
    "    install_agg.columns = ['SK_ID_CURR', 'INST_DPD_MEAN', 'INST_DPD_SUM', 'INST_PAYMENT_MEAN', 'INST_PAYMENT_SUM']\n",
    "    df_main = df_main.merge(install_agg, on='SK_ID_CURR', how='left')\n",
    "    del install, install_agg; gc.collect()\n",
    "    \n",
    "    # POS_CASH Balance\n",
    "    pos = pd.read_csv(DATA_ROOT + 'POS_CASH_balance.csv'); pos = downcast_dtypes(pos)\n",
    "    pos_agg = pos.groupby('SK_ID_CURR')[['CNT_INSTALMENT_FUTURE']].agg(['min', 'max', 'mean']).reset_index() \n",
    "    pos_agg.columns = ['SK_ID_CURR', 'POS_INST_FUTURE_MIN', 'POS_INST_FUTURE_MAX', 'POS_INST_FUTURE_MEAN']\n",
    "    df_main = df_main.merge(pos_agg, on='SK_ID_CURR', how='left')\n",
    "    del pos, pos_agg; gc.collect()\n",
    "\n",
    "    # Credit Card Balance\n",
    "    cc = pd.read_csv(DATA_ROOT + 'credit_card_balance.csv'); cc = downcast_dtypes(cc)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR')[['AMT_BALANCE']].agg(['mean', 'max']).reset_index()\n",
    "    cc_agg.columns = ['SK_ID_CURR', 'CC_BALANCE_MEAN', 'CC_BALANCE_MAX']\n",
    "    df_main = df_main.merge(cc_agg, on='SK_ID_CURR', how='left')\n",
    "    del cc, cc_agg; gc.collect()\n",
    "    \n",
    "    return df_main\n",
    "\n",
    "# ====================================================================\n",
    "# 4. SCORING PIPELINE EXECUTION BLOCK (Runs Automatically)\n",
    "# ====================================================================\n",
    "\n",
    "try:\n",
    "    # --- STEP 1: LOAD TEST DATA & INITIAL CLEANING ---\n",
    "    df_full = pd.read_csv(MAIN_FILE)\n",
    "    df_full = downcast_dtypes(df_full)\n",
    "    df_full = feature_engineer_application_data(df_full)\n",
    "    df_full = pd.get_dummies(df_full)\n",
    "    \n",
    "    X_raw = df_full \n",
    "    del df_full; gc.collect()\n",
    "\n",
    "    # --- STEP 2: GENERATE FULL FEATURE SET ---\n",
    "    print(\"\\n--- STEP 2: GENERATE FULL FEATURE SET FOR SCORING ---\")\n",
    "    \n",
    "    df_current = X_raw.copy()\n",
    "    \n",
    "    # Execute all feature engineering functions in sequence\n",
    "    df_current = merge_bureau_data(df_current) \n",
    "    df_current = get_prev_application_features(df_current)\n",
    "    df_current = get_temporal_features(df_current)\n",
    "\n",
    "    # --- STEP 3: LOAD MODEL AND SCORE DATA (FEATURE RECONCILIATION) ---\n",
    "    print(\"\\n--- STEP 3: LOAD MODEL AND SCORE DATA ---\")\n",
    "    \n",
    "    # 1. Load the saved pipeline\n",
    "    print(f\"\\nLoading final production model from {MODEL_SAVE_PATH}...\")\n",
    "    final_pipeline = joblib.load(MODEL_SAVE_PATH)\n",
    "    \n",
    "    # CRITICAL FIX 1: Retrieve the 241 feature names from the saved model artifact\n",
    "    TRAIN_FEATURES_241 = list(final_pipeline['preprocessor'].named_steps['imputer'].feature_names_in_)\n",
    "    \n",
    "    # 2. Final data preparation: Drop SK_ID_CURR and REINDEX against the 241 list\n",
    "    X_test_final = df_current.drop(columns='SK_ID_CURR', errors='ignore')\n",
    "    \n",
    "    # CRITICAL FIX 2: Reindex against the 241 features from training. \n",
    "    X_test_final = X_test_final.reindex(columns=TRAIN_FEATURES_241, fill_value=0)\n",
    "    \n",
    "    # 3. Predict probabilities (the final risk score)\n",
    "    y_pred_proba = final_pipeline.predict_proba(X_test_final.values)[:, 1] \n",
    "\n",
    "    # 4. Save the final output\n",
    "    \n",
    "    # CRITICAL FIX 3: Create the submission directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True) \n",
    "    \n",
    "    output_df = pd.DataFrame({\n",
    "        'SK_ID_CURR': df_current['SK_ID_CURR'],\n",
    "        'TARGET': y_pred_proba\n",
    "    })\n",
    "\n",
    "    output_df.to_csv(OUTPUT_PATH, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Production Scoring Complete. {len(output_df)} risk scores generated.\")\n",
    "    print(f\"Results saved to {OUTPUT_PATH} (File: credit_risk_scores.csv)\")\n",
    "    del df_current, X_test_final; gc.collect()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nðŸ›‘ FATAL ERROR DURING SCORING PIPELINE EXECUTION: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbf8e8-26fb-449c-8dd6-b6cd1204c1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
